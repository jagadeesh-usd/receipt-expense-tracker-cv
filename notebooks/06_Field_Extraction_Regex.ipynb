{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagadeesh-usd/receipt-expense-tracker-cv/blob/jaga-dev/notebooks/06_Field_Extraction_Regex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-cell"
      },
      "source": [
        "## Automated Expense Extraction - Receipt Parsing Using YOLO and OCR\n",
        "### Rule-Based Field Extraction (Regex)\n",
        "\n",
        "### Objective\n",
        "Apply **Regular Expressions (Regex)** and heuristic rules to the raw text generated by our Baseline OCR engines (EasyOCR & Tesseract). This step simulates a traditional \"Template-Free\" parsing approach to establish how well standard algorithms perform without Computer Vision localization.\n",
        "\n",
        "### Methodology\n",
        "1.  **Input:** JSON output files from Module 04 (EasyOCR) and Module 05 (Tesseract).\n",
        "2.  **Extraction Logic:**\n",
        "    * **Vendor:** Uses a naive heuristic (assuming the first non-generic line of text is the Vendor Name).\n",
        "    * **Date:** scans for standard date patterns (e.g., `DD/MM/YYYY`, `YYYY-MM-DD`, `DD-Mon-YYYY`).\n",
        "    * **Total Amount:** Searches for the largest number with two decimal places (standard currency regex).\n",
        "3.  **Process:** Iterates through all test receipts and applies these rules to the unstructured text blob.\n",
        "4.  **Output:** Structured CSV files (`processed/extracted/...`) ready for accuracy benchmarking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "#### Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mTESU6-3fVsj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Optional, Dict, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports",
        "outputId": "2d63dc2b-7c6f-4402-bfa4-9b20f757d3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Google Colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    # Mount Google Drive (for Colab)\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Set DATA_PATH for Google Drive\n",
        "    DATA_PATH = Path('/content/drive/MyDrive/data')\n",
        "else:\n",
        "    # Set DATA_PATH for local environment\n",
        "    DATA_PATH = Path('../data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config",
        "outputId": "cdba1ef1-4a37-4242-ea42-748d9a74bdae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FIELD EXTRACTION CONFIGURATION\n",
            "======================================================================\n",
            "Data Path: /content/drive/MyDrive/data\n",
            "\n",
            "OCR Engines to Process: 2\n",
            "  â€¢ EasyOCR\n",
            "  â€¢ Tesseract\n",
            "\n",
            "Splits: train, test\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# CONFIGURATION - Choose which OCR engine(s) to process\n",
        "\n",
        "# Set to True to process that OCR engine's results\n",
        "PROCESS_EASYOCR = True      # Extract fields from EasyOCR results\n",
        "PROCESS_TESSERACT = True    # Extract fields from Tesseract results\n",
        "\n",
        "# Splits to process (train, test, or both)\n",
        "SPLITS = [\"train\", \"test\"]\n",
        "\n",
        "# Path Configuration\n",
        "\n",
        "OCR_ENGINES = []\n",
        "if PROCESS_EASYOCR:\n",
        "    OCR_ENGINES.append({\n",
        "        'name': 'EasyOCR',\n",
        "        'input_dir': DATA_PATH / 'processed/ocr',\n",
        "        'output_dir': DATA_PATH / 'processed/extracted/ocr'\n",
        "    })\n",
        "\n",
        "if PROCESS_TESSERACT:\n",
        "    OCR_ENGINES.append({\n",
        "        'name': 'Tesseract',\n",
        "        'input_dir': DATA_PATH / 'processed/tesseract_ocr',\n",
        "        'output_dir': DATA_PATH / 'processed/extracted/tesseract_ocr'\n",
        "    })\n",
        "\n",
        "# Verify configuration\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FIELD EXTRACTION CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Data Path: {DATA_PATH}\")\n",
        "print(f\"\\nOCR Engines to Process: {len(OCR_ENGINES)}\")\n",
        "for engine in OCR_ENGINES:\n",
        "    print(f\"  â€¢ {engine['name']}\")\n",
        "print(f\"\\nSplits: {', '.join(SPLITS)}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "if not OCR_ENGINES:\n",
        "    print(\"WARNING: No OCR engines selected! Set PROCESS_EASYOCR or PROCESS_TESSERACT to True.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patterns-section"
      },
      "source": [
        "#### 1. Regex Patterns & Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "regex-patterns",
        "outputId": "77691c9d-12f3-4c89-c4f7-8d33066d5c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Regex patterns loaded\n"
          ]
        }
      ],
      "source": [
        "# REGEX PATTERNS\n",
        "\n",
        "# Matches currency-like numbers (e.g., 1,234.56 or 12.50)\n",
        "CURRENCY_NUM_RE = re.compile(r'([0-9]{1,3}(?:[,][0-9]{3})*(?:\\.[0-9]{2})|[0-9]+(?:\\.[0-9]{2}))')\n",
        "\n",
        "# Gap-Tolerant Total Regex\n",
        "# Matches \"Total\", \"Amount\", etc., followed by up to 25 chars of \"noise\", then the number\n",
        "AMOUNT_LABEL_RE = re.compile(\n",
        "    r'(?i)\\b(total|amount|grand total|grand|balance|invoice total|amount due|nett|payable)'\n",
        "    r'(?:[^0-9\\n\\-\\+]{0,25})'  # Allow gap of up to 25 non-digit chars\n",
        "    r'\\s*([0-9,]+\\.\\d{2})'     # The Amount\n",
        ")\n",
        "\n",
        "# Blacklist for vendor guessing (common receipt words to ignore)\n",
        "VENDOR_BLACKLIST = [\n",
        "    \"total\", \"subtotal\", \"amount\", \"gst\", \"tax\", \"invoice\", \"cash\", \"change\",\n",
        "    \"tel\", \"fax\", \"receipt\", \"date\", \"time\", \"document\", \"table\", \"pax\", \"order\",\n",
        "    \"thank\", \"welcome\", \"regards\", \"bill\", \"payment\"\n",
        "]\n",
        "\n",
        "print(\"âœ“ Regex patterns loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "helper-functions",
        "outputId": "f63f0dca-29bd-456e-ed32-38e0830e70ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Helper functions loaded\n"
          ]
        }
      ],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "def normalize_number_str(s: str) -> Optional[float]:\n",
        "    \"\"\"Turn common currency-like strings into float or None.\"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "    s = s.replace(',', '').strip()\n",
        "    s = re.sub(r'[^0-9\\.\\-]', '', s)  # Remove currency symbols/letters\n",
        "    try:\n",
        "        return float(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def normalize_date_ocr(date_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Heuristic cleanup for common OCR date errors before parsing.\n",
        "    Example: 25/4212018 -> 25/04/2018 (separator glitch)\n",
        "    \"\"\"\n",
        "    if not date_str:\n",
        "        return \"\"\n",
        "    s = date_str.strip()\n",
        "\n",
        "    # Replace common letter-swaps for numbers\n",
        "    s = s.replace('O', '0').replace('o', '0')\n",
        "    s = s.replace('l', '1').replace('I', '1')\n",
        "\n",
        "    # Broken separators in OCR (common in SROIE dataset)\n",
        "    if \"/42120\" in s:\n",
        "        s = s.replace(\"/42120\", \"/04/20\")\n",
        "    if \"/420\" in s:\n",
        "        s = s.replace(\"/420\", \"/04/20\")\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def find_date_in_text(full_text: str) -> Optional[str]:\n",
        "    \"\"\"Finds date candidates and tries to parse them into YYYY-MM-DD.\"\"\"\n",
        "    if not full_text:\n",
        "        return None\n",
        "\n",
        "    # Pattern 1: Standard dates (25/04/2018, 2018-04-25, 25.04.18)\n",
        "    date_pattern = re.compile(r'\\b(\\d{1,2}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{2,4})\\b')\n",
        "    # Pattern 2: Corrupted OCR dates (e.g., 25/4212018 where separators failed)\n",
        "    broken_pattern = re.compile(r'\\b(\\d{1,2}/\\d{5,8})\\b')\n",
        "\n",
        "    # Gather all candidates\n",
        "    candidates = date_pattern.findall(full_text) + broken_pattern.findall(full_text)\n",
        "\n",
        "    for raw in candidates:\n",
        "        clean = normalize_date_ocr(raw)\n",
        "        # Try parsing various common receipt formats\n",
        "        for fmt in (\"%d/%m/%Y\", \"%d-%m-%Y\", \"%d.%m.%Y\",\n",
        "                    \"%d/%m/%y\", \"%d-%m-%y\", \"%Y-%m-%d\"):\n",
        "            try:\n",
        "                dt = datetime.strptime(clean, fmt)\n",
        "                return dt.strftime(\"%Y-%m-%d\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return None\n",
        "\n",
        "\n",
        "def find_total_in_text(full_text: str) -> Optional[float]:\n",
        "    \"\"\"Finds total amount using Label-First strategy, then Fallback to last number.\"\"\"\n",
        "    if not full_text:\n",
        "        return None\n",
        "\n",
        "    # 1. Label-based search (Best accuracy)\n",
        "    matches = list(AMOUNT_LABEL_RE.finditer(full_text))\n",
        "    if matches:\n",
        "        # Use the last labeled total found (usually the Grand Total at bottom)\n",
        "        for m in reversed(matches):\n",
        "            val = m.group(2)\n",
        "            num = normalize_number_str(val)\n",
        "            if num is not None:\n",
        "                return num\n",
        "\n",
        "    # 2. Fallback: Last currency-formatted number in text\n",
        "    # (Useful if the word \"Total\" is missing or OCR failed on the label)\n",
        "    nums = CURRENCY_NUM_RE.findall(full_text)\n",
        "    if nums:\n",
        "        val = nums[-1]\n",
        "        return normalize_number_str(val)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def guess_vendor_from_lines(lines: List[str]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Generic heuristic to find vendor name.\n",
        "    Returns the first line that looks like a name (has letters) and isn't blacklisted.\n",
        "    \"\"\"\n",
        "    if not lines:\n",
        "        return None\n",
        "\n",
        "    # Check only the top header section (first 8 lines)\n",
        "    for line in lines[:8]:\n",
        "        line = line.strip()\n",
        "        if len(line) < 3:\n",
        "            continue  # Skip tiny noise\n",
        "\n",
        "        low = line.lower()\n",
        "\n",
        "        # Skip blacklisted generic words (Receipt, Tax Invoice, etc.)\n",
        "        if any(b in low for b in VENDOR_BLACKLIST):\n",
        "            continue\n",
        "\n",
        "        # If line contains letters, it's likely the Vendor Name\n",
        "        if any(c.isalpha() for c in line):\n",
        "            return line\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_fields_from_ocr_result(ocr_result: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Main extraction function.\n",
        "    Takes OCR result dict and returns extracted vendor, date, total.\n",
        "    \"\"\"\n",
        "    full_text = (ocr_result.get(\"full_text\") or \"\").strip()\n",
        "    lines = ocr_result.get(\"lines\") or []\n",
        "\n",
        "    vendor = guess_vendor_from_lines(lines)\n",
        "    date = find_date_in_text(full_text)\n",
        "    total = find_total_in_text(full_text)\n",
        "\n",
        "    return {\n",
        "        \"vendor\": vendor,\n",
        "        \"date\": date,\n",
        "        \"total\": total,\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"âœ“ Helper functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extraction-section"
      },
      "source": [
        "#### 3. Field Extraction Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extraction-pipeline",
        "outputId": "50cc76d7-0b6c-4527-c74d-8911bdb894b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Extraction pipeline ready\n"
          ]
        }
      ],
      "source": [
        "# EXTRACTION PIPELINE\n",
        "\n",
        "def process_ocr_engine(engine_config: dict, splits: List[str]):\n",
        "    \"\"\"\n",
        "    Process one OCR engine's results across multiple splits.\n",
        "\n",
        "    Args:\n",
        "        engine_config: Dict with 'name', 'input_dir', 'output_dir'\n",
        "        splits: List of splits to process (e.g., ['train', 'test'])\n",
        "    \"\"\"\n",
        "    engine_name = engine_config['name']\n",
        "    input_dir = engine_config['input_dir']\n",
        "    output_dir = engine_config['output_dir']\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing: {engine_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Input:  {input_dir}\")\n",
        "    print(f\"Output: {output_dir}\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    total_processed = 0\n",
        "\n",
        "    for split in splits:\n",
        "        src_dir = input_dir / split\n",
        "        out_csv = output_dir / f\"{split}_extracted.csv\"\n",
        "\n",
        "        if not src_dir.exists():\n",
        "            print(f\"\\n {split}: Directory not found: {src_dir}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nðŸ“‚ Processing {split}...\")\n",
        "\n",
        "        rows = []\n",
        "        json_files = sorted(src_dir.glob(\"*.json\"))\n",
        "\n",
        "        if not json_files:\n",
        "            print(f\" No JSON files found in {src_dir}\")\n",
        "            continue\n",
        "\n",
        "        # Process each OCR result\n",
        "        for jfile in json_files:\n",
        "            try:\n",
        "                with open(jfile, \"r\", encoding=\"utf-8\") as f:\n",
        "                    ocr_result = json.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\" Failed to read {jfile.name}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Extract fields\n",
        "            extracted = extract_fields_from_ocr_result(ocr_result)\n",
        "\n",
        "            rows.append({\n",
        "                \"image\": jfile.stem,\n",
        "                \"vendor\": extracted.get(\"vendor\"),\n",
        "                \"date\": extracted.get(\"date\"),\n",
        "                \"total\": extracted.get(\"total\")\n",
        "            })\n",
        "\n",
        "        # Write to CSV\n",
        "        with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvf:\n",
        "            writer = csv.DictWriter(csvf, fieldnames=[\"image\", \"vendor\", \"date\", \"total\"])\n",
        "            writer.writeheader()\n",
        "            for row in rows:\n",
        "                writer.writerow(row)\n",
        "\n",
        "        print(f\"   âœ“ Saved: {out_csv}\")\n",
        "        print(f\"   âœ“ Records: {len(rows)}\")\n",
        "        total_processed += len(rows)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{engine_name} Complete: {total_processed} total records processed\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "\n",
        "print(\"âœ“ Extraction pipeline ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-section"
      },
      "source": [
        "#### 4. Run Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "run-extraction",
        "outputId": "951748d4-9493-4723-830e-c6e56b90c2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################################################\n",
            "# STARTING FIELD EXTRACTION\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "Processing: EasyOCR\n",
            "======================================================================\n",
            "Input:  /content/drive/MyDrive/data/processed/ocr\n",
            "Output: /content/drive/MyDrive/data/processed/extracted/ocr\n",
            "\n",
            "ðŸ“‚ Processing train...\n",
            "   âœ“ Saved: /content/drive/MyDrive/data/processed/extracted/ocr/train_extracted.csv\n",
            "   âœ“ Records: 626\n",
            "\n",
            "ðŸ“‚ Processing test...\n",
            "   âœ“ Saved: /content/drive/MyDrive/data/processed/extracted/ocr/test_extracted.csv\n",
            "   âœ“ Records: 347\n",
            "\n",
            "======================================================================\n",
            "EasyOCR Complete: 973 total records processed\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Processing: Tesseract\n",
            "======================================================================\n",
            "Input:  /content/drive/MyDrive/data/processed/tesseract_ocr\n",
            "Output: /content/drive/MyDrive/data/processed/extracted/tesseract_ocr\n",
            "\n",
            "ðŸ“‚ Processing train...\n",
            "   âœ“ Saved: /content/drive/MyDrive/data/processed/extracted/tesseract_ocr/train_extracted.csv\n",
            "   âœ“ Records: 626\n",
            "\n",
            "ðŸ“‚ Processing test...\n",
            "   âœ“ Saved: /content/drive/MyDrive/data/processed/extracted/tesseract_ocr/test_extracted.csv\n",
            "   âœ“ Records: 347\n",
            "\n",
            "======================================================================\n",
            "Tesseract Complete: 973 total records processed\n",
            "======================================================================\n",
            "\n",
            "######################################################################\n",
            "# EXTRACTION COMPLETE\n",
            "######################################################################\n",
            "\n",
            " Processed 2 OCR engine(s)\n",
            " Output saved to: /content/drive/MyDrive/data/processed/extracted\n"
          ]
        }
      ],
      "source": [
        "# RUN EXTRACTION FOR ALL CONFIGURED ENGINES\n",
        "\n",
        "if not OCR_ENGINES:\n",
        "    print(\"No OCR engines configured. Please set PROCESS_EASYOCR or PROCESS_TESSERACT to True.\")\n",
        "else:\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"# STARTING FIELD EXTRACTION\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    for engine_config in OCR_ENGINES:\n",
        "        try:\n",
        "            process_ocr_engine(engine_config, SPLITS)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing {engine_config['name']}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"# EXTRACTION COMPLETE\")\n",
        "    print(\"#\"*70)\n",
        "    print(f\"\\n Processed {len(OCR_ENGINES)} OCR engine(s)\")\n",
        "    print(f\" Output saved to: {DATA_PATH / 'processed/extracted'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary-section"
      },
      "source": [
        "#### 5. Summary & Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "summary",
        "outputId": "9fc8cade-dc2d-4993-b123-274d3bb430aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EXTRACTION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "EasyOCR:\n",
            "  â€¢ train: 626 records â†’ /content/drive/MyDrive/data/processed/extracted/ocr/train_extracted.csv\n",
            "  â€¢ test: 347 records â†’ /content/drive/MyDrive/data/processed/extracted/ocr/test_extracted.csv\n",
            "\n",
            "Tesseract:\n",
            "  â€¢ train: 626 records â†’ /content/drive/MyDrive/data/processed/extracted/tesseract_ocr/train_extracted.csv\n",
            "  â€¢ test: 347 records â†’ /content/drive/MyDrive/data/processed/extracted/tesseract_ocr/test_extracted.csv\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Field extraction complete!\n"
          ]
        }
      ],
      "source": [
        "# SUMMARY\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXTRACTION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for engine_config in OCR_ENGINES:\n",
        "    engine_name = engine_config['name']\n",
        "    output_dir = engine_config['output_dir']\n",
        "\n",
        "    print(f\"\\n{engine_name}:\")\n",
        "\n",
        "    for split in SPLITS:\n",
        "        csv_file = output_dir / f\"{split}_extracted.csv\"\n",
        "\n",
        "        if csv_file.exists():\n",
        "            # Count rows\n",
        "            with open(csv_file, 'r', encoding='utf-8') as f:\n",
        "                row_count = sum(1 for line in f) - 1  # Subtract header\n",
        "\n",
        "            print(f\"  â€¢ {split}: {row_count} records â†’ {csv_file}\")\n",
        "        else:\n",
        "            print(f\"  â€¢ {split}: Not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nField extraction complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample-section"
      },
      "source": [
        "#### 6. Sample Output Preview  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sample-preview",
        "outputId": "3169b641-cdc8-442b-9ae0-720608a4508e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "SAMPLE EXTRACTED FIELDS\n",
            "======================================================================\n",
            "\n",
            "EasyOCR - Test Set (first 5 records):\n",
            "----------------------------------------------------------------------\n",
            "          image                vendor        date  total\n",
            "0  X00016469670                   tan  2019-01-15  193.0\n",
            "1  X00016469671                   tan  2019-01-02  170.0\n",
            "2  X51005200931  PERNIAGAAN ZHENG HUI  2018-02-09  436.2\n",
            "3  X51005230605  PEIROH BKT LANJAN SB  2018-02-01    6.0\n",
            "4  X51005230616               4 psez.  2018-01-18   38.9\n",
            "\n",
            "Statistics:\n",
            "  Total records: 347\n",
            "  Vendor extracted: 346 (99.7%)\n",
            "  Date extracted: 233 (67.1%)\n",
            "  Total extracted: 333 (96.0%)\n",
            "\n",
            "Tesseract - Test Set (first 5 records):\n",
            "----------------------------------------------------------------------\n",
            "          image                               vendor        date   total\n",
            "0  X00016469670                         tan chay yee         NaN  193.00\n",
            "1  X00016469671                         tan chay yee  2019-01-02  170.00\n",
            "2  X51005200931                               hore 2  2078-02-09   24.69\n",
            "3  X51005230605  : 03-6156 8757 Co No: 001083069-M 4         NaN    1.90\n",
            "4  X51005230616     Gerbang Alaf Restaurants Sdn Bhd  2018-01-18   38.90\n",
            "\n",
            "Statistics:\n",
            "  Total records: 347\n",
            "  Vendor extracted: 345 (99.4%)\n",
            "  Date extracted: 230 (66.3%)\n",
            "  Total extracted: 339 (97.7%)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# PREVIEW SAMPLE RESULTS\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE EXTRACTED FIELDS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for engine_config in OCR_ENGINES:\n",
        "    engine_name = engine_config['name']\n",
        "    output_dir = engine_config['output_dir']\n",
        "\n",
        "    # Show sample from test set\n",
        "    csv_file = output_dir / \"test_extracted.csv\"\n",
        "\n",
        "    if csv_file.exists():\n",
        "        print(f\"\\n{engine_name} - Test Set (first 5 records):\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        df = pd.read_csv(csv_file)\n",
        "        print(df.head())\n",
        "\n",
        "        # Statistics\n",
        "        print(f\"\\nStatistics:\")\n",
        "        print(f\"  Total records: {len(df)}\")\n",
        "        print(f\"  Vendor extracted: {df['vendor'].notna().sum()} ({df['vendor'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "        print(f\"  Date extracted: {df['date'].notna().sum()} ({df['date'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "        print(f\"  Total extracted: {df['total'].notna().sum()} ({df['total'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"\\n{engine_name}: No test results found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}