{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FyLD_MKUMZ5"
   },
   "source": [
    "# Automated Expense Extraction - Receipt Parsing Using YOLO and OCR\n",
    "### Adaptive Image Preprocessing\n",
    "\n",
    "#### Objectives\n",
    "Based on EDA findings, implement **adaptive preprocessing** that:\n",
    "1. Classifies images by quality (faint/normal/shadowed)\n",
    "2. Applies appropriate preprocessing strategy per image\n",
    "3. Optimizes for both OCR readability and YOLO detection\n",
    "4. Generates processed images for downstream tasks\n",
    "\n",
    "#### Preprocessing Strategies\n",
    "\n",
    "##### Strategy 1: Light Preprocessing (Faint Images, STD < 30)\n",
    "- **Goal:** Enhance contrast without over-processing\n",
    "- **Steps:**\n",
    "  1. Minimal denoising (Bilateral filter)\n",
    "  2. CLAHE with moderate clip limit\n",
    "  \n",
    "##### Strategy 2: Medium Preprocessing (Normal Images, STD 30-55)\n",
    "- **Goal:** Standard enhancement for typical receipts\n",
    "- **Steps:**\n",
    "  1. Gaussian blur for noise reduction\n",
    "  2. CLAHE for contrast improvement\n",
    "  \n",
    "##### Strategy 3: Heavy Preprocessing (Shadowed Images, STD > 55)\n",
    "- **Goal:** Remove shadows and background\n",
    "- **Steps:**\n",
    "  1. Gaussian blur\n",
    "  2. Adaptive thresholding for background removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBtO1TWlUMZ7"
   },
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27597,
     "status": "ok",
     "timestamp": 1765035714954,
     "user": {
      "displayName": "Jagadeesh Sellappan",
      "userId": "07215682532086792256"
     },
     "user_tz": -330
    },
    "id": "ey6bgEhNUMZ7",
    "outputId": "9a19700c-b5f8-4969-e2d5-17a1f9b9ee6d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine if running in Google Colab and set paths accordingly\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_PATH = Path('/content/drive/MyDrive/data')\n",
    "else:\n",
    "    DATA_PATH = Path('../data')\n",
    "    print(\"Not running in Google Colab. Skipping Google Drive mounting.\")\n",
    "\n",
    "# Configuration\n",
    "RAW_DIR = DATA_PATH / \"raw/SROIE2019\"\n",
    "PROCESSED_DIR = DATA_PATH / \"processed/SROIE2019\"\n",
    "\n",
    "# Verify paths\n",
    "for directory in [RAW_DIR, PROCESSED_DIR]:\n",
    "    print(f\"{'Raw' if directory == RAW_DIR else 'Output'} directory: {directory}\")\n",
    "    print(f\"{'Raw' if directory == RAW_DIR else 'Output'} dir exists: {directory.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KHdrQiAUMZ8"
   },
   "source": [
    "## 1. Core Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1765035722016,
     "user": {
      "displayName": "Jagadeesh Sellappan",
      "userId": "07215682532086792256"
     },
     "user_tz": -330
    },
    "id": "wV4WEWllUMZ8"
   },
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "def load_image(path):\n",
    "    \"\"\"\n",
    "    Load image from path with error handling.\n",
    "\n",
    "    Args:\n",
    "        path: Path to image file\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Loaded image in BGR format\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If image cannot be loaded\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Could not load image: {path}\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def to_grayscale(img):\n",
    "    \"\"\"\n",
    "    Convert BGR image to grayscale.\n",
    "\n",
    "    Args:\n",
    "        img: BGR image\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Grayscale image\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def classify_receipt(gray):\n",
    "    \"\"\"\n",
    "    Classify receipt quality based on contrast (standard deviation).\n",
    "\n",
    "    Classification Rules (from EDA):\n",
    "    - std < 30: Faint/low contrast → Light processing\n",
    "    - 30 ≤ std ≤ 55: Normal → Medium processing\n",
    "    - std > 55: Shadowed/high contrast → Heavy processing\n",
    "\n",
    "    Args:\n",
    "        gray: Grayscale image\n",
    "\n",
    "    Returns:\n",
    "        str: Quality category ('light', 'medium', or 'heavy')\n",
    "    \"\"\"\n",
    "    std = np.std(gray)\n",
    "    mean = np.mean(gray)\n",
    "\n",
    "    if std < 30:\n",
    "        return 'light', std, mean\n",
    "    elif 30 <= std <= 55:\n",
    "        return 'medium', std, mean\n",
    "    else:\n",
    "        return 'heavy', std, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sP5UftbUMZ8"
   },
   "source": [
    "## 2. Adaptive Preprocessing Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1765035724355,
     "user": {
      "displayName": "Jagadeesh Sellappan",
      "userId": "07215682532086792256"
     },
     "user_tz": -330
    },
    "id": "WGF0XzWaUMZ9"
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING STRATEGIES\n",
    "\n",
    "def preprocess_light(gray):\n",
    "    \"\"\"\n",
    "    Light preprocessing for faint/low-contrast images.\n",
    "\n",
    "    Strategy:\n",
    "    1. Minimal denoising with bilateral filter (preserves edges)\n",
    "    2. Moderate CLAHE to enhance faint text without over-processing\n",
    "\n",
    "    Args:\n",
    "        gray: Grayscale image\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Enhanced grayscale image\n",
    "    \"\"\"\n",
    "    # Step 1: Minimal denoising (preserves edges better than Gaussian)\n",
    "    denoised = cv2.bilateralFilter(gray, d=5, sigmaColor=50, sigmaSpace=50)\n",
    "\n",
    "    # Step 2: CLAHE with moderate clip limit for faint text\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.8, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(denoised)\n",
    "\n",
    "    return enhanced\n",
    "\n",
    "\n",
    "def preprocess_medium(gray):\n",
    "    \"\"\"\n",
    "    Medium preprocessing for normal-quality images.\n",
    "\n",
    "    Strategy:\n",
    "    1. Gaussian blur for standard noise reduction\n",
    "    2. CLAHE with balanced parameters\n",
    "\n",
    "    Args:\n",
    "        gray: Grayscale image\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Enhanced grayscale image\n",
    "    \"\"\"\n",
    "    # Step 1: Standard Gaussian blur for noise reduction\n",
    "    # Kernel (3,3): Small kernel to reduce noise without losing detail\n",
    "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    # Step 2: Balanced CLAHE for contrast enhancement\n",
    "    # clipLimit=2.5: Moderate enhancement without artifacts\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(blurred)\n",
    "\n",
    "    return enhanced\n",
    "\n",
    "\n",
    "def preprocess_heavy(gray):\n",
    "    \"\"\"\n",
    "    Heavy preprocessing for shadowed/high-contrast images.\n",
    "\n",
    "    Strategy:\n",
    "    1. Gaussian blur for noise reduction\n",
    "    2. Adaptive thresholding to remove shadows and backgrounds\n",
    "\n",
    "    Args:\n",
    "        gray: Grayscale image\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binary image with background removed\n",
    "    \"\"\"\n",
    "    # Step 1: More aggressive blur for shadow smoothing\n",
    "    # Kernel (5,5): Larger kernel to smooth shadows\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Step 2: Adaptive thresholding for shadow/background removal\n",
    "    # ADAPTIVE_THRESH_GAUSSIAN_C: Better for uneven illumination\n",
    "    # blockSize=31: Large block to handle shadow variations\n",
    "    # C=5: Subtracted constant (fine-tuned for receipts)\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        blurred,\n",
    "        maxValue=255,\n",
    "        adaptiveMethod=cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        thresholdType=cv2.THRESH_BINARY,\n",
    "        blockSize=31,  \n",
    "        C=5\n",
    "    )\n",
    "\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mf7xbXQmUMZ9"
   },
   "source": [
    "## 3. Main Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1765035727133,
     "user": {
      "displayName": "Jagadeesh Sellappan",
      "userId": "07215682532086792256"
     },
     "user_tz": -330
    },
    "id": "a37WdekfUMZ9"
   },
   "outputs": [],
   "source": [
    "# MAIN PIPELINE\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\"\n",
    "    Main preprocessing controller that routes images to appropriate strategy.\n",
    "\n",
    "    Workflow:\n",
    "    1. Load image\n",
    "    2. Convert to grayscale\n",
    "    3. Classify quality\n",
    "    4. Apply appropriate preprocessing\n",
    "\n",
    "    Args:\n",
    "        img_path: Path to input image\n",
    "\n",
    "    Returns:\n",
    "        tuple: (processed_image, quality_category, std, mean)\n",
    "    \"\"\"\n",
    "    # Load and convert to grayscale\n",
    "    img = load_image(img_path)\n",
    "    gray = to_grayscale(img)\n",
    "\n",
    "    # Classify image quality\n",
    "    category, std, mean = classify_receipt(gray)\n",
    "\n",
    "    # Route to appropriate preprocessing strategy\n",
    "    if category == 'light':\n",
    "        processed = preprocess_light(gray)\n",
    "    elif category == 'medium':\n",
    "        processed = preprocess_medium(gray)\n",
    "    else:  # heavy\n",
    "        processed = preprocess_heavy(gray)\n",
    "\n",
    "    return processed, category, std, mean\n",
    "\n",
    "\n",
    "def save_preprocessed_image(img_path, output_path):\n",
    "    \"\"\"\n",
    "    Process and save a single image.\n",
    "\n",
    "    Args:\n",
    "        img_path: Path to input image\n",
    "        output_path: Path to save processed image\n",
    "\n",
    "    Returns:\n",
    "        dict: Processing metadata (category, std, mean, success)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Process image\n",
    "        processed, category, std, mean = preprocess_image(img_path)\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save processed image\n",
    "        cv2.imwrite(str(output_path), processed)\n",
    "\n",
    "        return {\n",
    "            'filename': Path(img_path).name,\n",
    "            'category': category,\n",
    "            'std': std,\n",
    "            'mean': mean,\n",
    "            'success': True\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'filename': Path(img_path).name,\n",
    "            'category': 'error',\n",
    "            'std': 0,\n",
    "            'mean': 0,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qhclb6K9UMZ9"
   },
   "source": [
    "## 4. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778,
     "referenced_widgets": [
      "1f2675c6773243f8872ac8d74af66f21",
      "1f605f6b91d9436baecb445dea8623e7",
      "ce9b732d3fe74065b8e20cd69591d962",
      "a31f7fb3926b43b79b9c18bde98c5f6f",
      "5fe50a58f76b41c8b86e0afd261b1256",
      "79569c2c73a4443fb88d5da778cdd1e7",
      "e647ddadf6634f90a8459937f0b7a15c",
      "b7c6238df06f40f2a0ebdde9b8f4f493",
      "e15e39dafbad46559c9f5c8344743a31",
      "cf0a5c2f9cbb495aad226dfc100edcde",
      "9fd3d2d00eec45cebbf3d3e6da01e4c2",
      "57b6796122464966a714caefde196f06",
      "0039177b685b4d2681bb47cef4040f9c",
      "ed0b284af0354e368980338653ac7de3",
      "da1b7397777944b48f031a35c950aa5f",
      "b23c8acfa96448939b87a924d5e9b21b",
      "ae6f6fa5901d4095a9f63a179efdd194",
      "89b6466e829c4f94862128d44378b31e",
      "aaf7400bc4d5466d954bc6443928f28b",
      "c13fcabcda6a4522bfef0ef9fdfd4b81",
      "70690091fcbc4275b82628436f928099",
      "1c9f6f3ee5ea4205b99176242eb26e3a"
     ]
    },
    "executionInfo": {
     "elapsed": 941787,
     "status": "ok",
     "timestamp": 1765036672090,
     "user": {
      "displayName": "Jagadeesh Sellappan",
      "userId": "07215682532086792256"
     },
     "user_tz": -330
    },
    "id": "yrC9bxfzUMZ-",
    "outputId": "ae5813bf-a5fc-4cb3-f2ce-2a916378641a"
   },
   "outputs": [],
   "source": [
    "def process_dataset(splits=['train', 'test']):\n",
    "    \"\"\"\n",
    "    Process all images in the dataset.\n",
    "\n",
    "    Args:\n",
    "        splits: List of splits to process (e.g., ['train', 'test'])\n",
    "\n",
    "    Returns:\n",
    "        dict: Processing statistics per split\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    for split in splits:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {split.upper()} set\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Define paths\n",
    "        input_dir = RAW_DIR / split / \"img\"\n",
    "        output_dir = PROCESSED_DIR / split\n",
    "\n",
    "        # Get all image files\n",
    "        image_files = list(input_dir.glob(\"*.jpg\")) + list(input_dir.glob(\"*.png\"))\n",
    "        print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "        # Process images\n",
    "        results = []\n",
    "        for img_path in tqdm(image_files, desc=f\"Processing {split}\"):\n",
    "            output_path = output_dir / img_path.name\n",
    "            result = save_preprocessed_image(str(img_path), str(output_path))\n",
    "            results.append(result)\n",
    "\n",
    "        # Calculate statistics\n",
    "        successful = [r for r in results if r['success']]\n",
    "        failed = [r for r in results if not r['success']]\n",
    "\n",
    "        categories = {}\n",
    "        for r in successful:\n",
    "            cat = r['category']\n",
    "            categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "        stats[split] = {\n",
    "            'total': len(results),\n",
    "            'successful': len(successful),\n",
    "            'failed': len(failed),\n",
    "            'categories': categories\n",
    "        }\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\n{split.upper()} Summary:\")\n",
    "        print(f\"  Total: {stats[split]['total']}\")\n",
    "        print(f\"  Successful: {stats[split]['successful']}\")\n",
    "        print(f\"  Failed: {stats[split]['failed']}\")\n",
    "        print(f\"\\n  Category Distribution:\")\n",
    "        for cat, count in categories.items():\n",
    "            percentage = (count / len(successful)) * 100\n",
    "            print(f\"    {cat}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        if failed:\n",
    "            print(f\"\\n  ⚠️ Failed images:\")\n",
    "            for r in failed[:5]:  # Show first 5 errors\n",
    "                print(f\"    - {r['filename']}: {r.get('error', 'Unknown error')}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Execute batch processing\n",
    "print(\"Starting batch preprocessing...\\n\")\n",
    "processing_stats = process_dataset(['train', 'test'])\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"✅ PREPROCESSING COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nProcessed images saved to: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m587x5zAUMZ-"
   },
   "source": [
    "## 5. Visual Verification: Before vs After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9228,
     "status": "ok",
     "timestamp": 1765036707293,
     "user": {
      "displayName": "Jagadeesh Sellappan",
      "userId": "07215682532086792256"
     },
     "user_tz": -330
    },
    "id": "BWAIxuLfUMZ-",
    "outputId": "97e241bc-3109-45e8-f7cc-5e1489e44929"
   },
   "outputs": [],
   "source": [
    "def visualize_preprocessing_results(split='train', n_samples=6):\n",
    "    \"\"\"\n",
    "    Visualize original vs processed images with quality category labels.\n",
    "    \"\"\"\n",
    "    raw_dir = RAW_DIR / split / \"img\"\n",
    "    proc_dir = PROCESSED_DIR / split\n",
    "\n",
    "    # Get random samples\n",
    "    image_files = list(raw_dir.glob(\"*.jpg\"))[:n_samples]\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(12, 4*n_samples))\n",
    "    fig.suptitle(f'{split.upper()} Set: Raw vs Processed Images',\n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "    for idx, img_path in enumerate(image_files):\n",
    "        # Load raw image\n",
    "        raw_img = cv2.imread(str(img_path))\n",
    "        raw_rgb = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)\n",
    "        raw_gray = cv2.cvtColor(raw_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Get category\n",
    "        category, std, mean = classify_receipt(raw_gray)\n",
    "\n",
    "        # Load processed image\n",
    "        proc_path = proc_dir / img_path.name\n",
    "        proc_img = cv2.imread(str(proc_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Display\n",
    "        axes[idx, 0].imshow(raw_rgb)\n",
    "        axes[idx, 0].set_title(f'Original\\nSTD: {std:.1f}, Mean: {mean:.1f}', fontsize=10)\n",
    "        axes[idx, 0].axis('off')\n",
    "\n",
    "        axes[idx, 1].imshow(proc_img, cmap='gray')\n",
    "        axes[idx, 1].set_title(f'Processed ({category})\\n{img_path.name}', fontsize=10)\n",
    "        axes[idx, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "print(\"Visualizing preprocessing results...\\n\")\n",
    "visualize_preprocessing_results('train', n_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG0S12wLUMZ-"
   },
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "### Adaptive Processing Applied:\n",
    "- ✅ **Faint images:** Enhanced with CLAHE\n",
    "- ✅ **Normal images:** Balanced preprocessing\n",
    "- ✅ **Shadowed images:** Background removed via adaptive thresholding\n",
    "\n",
    "### Benefits for Downstream Tasks:\n",
    "1. **OCR (Tesseract/EasyOCR):** Cleaner text, better character recognition\n",
    "2. **YOLO Detection:** Enhanced contrast helps with bounding box detection\n",
    "3. **Consistency:** All images processed to similar quality level\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**YOLO Dataset Preparation:**\n",
    "- Notebook: `02_PrepareYOLO.ipynb`\n",
    "- Convert SROIE format (8-point boxes) to YOLO format\n",
    "- Create train/val splits\n",
    "- Generate YAML configuration\n",
    "- Use **processed images** as input"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0039177b685b4d2681bb47cef4040f9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae6f6fa5901d4095a9f63a179efdd194",
      "placeholder": "​",
      "style": "IPY_MODEL_89b6466e829c4f94862128d44378b31e",
      "value": "Processing test: 100%"
     }
    },
    "1c9f6f3ee5ea4205b99176242eb26e3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f2675c6773243f8872ac8d74af66f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f605f6b91d9436baecb445dea8623e7",
       "IPY_MODEL_ce9b732d3fe74065b8e20cd69591d962",
       "IPY_MODEL_a31f7fb3926b43b79b9c18bde98c5f6f"
      ],
      "layout": "IPY_MODEL_5fe50a58f76b41c8b86e0afd261b1256"
     }
    },
    "1f605f6b91d9436baecb445dea8623e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79569c2c73a4443fb88d5da778cdd1e7",
      "placeholder": "​",
      "style": "IPY_MODEL_e647ddadf6634f90a8459937f0b7a15c",
      "value": "Processing train: 100%"
     }
    },
    "57b6796122464966a714caefde196f06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0039177b685b4d2681bb47cef4040f9c",
       "IPY_MODEL_ed0b284af0354e368980338653ac7de3",
       "IPY_MODEL_da1b7397777944b48f031a35c950aa5f"
      ],
      "layout": "IPY_MODEL_b23c8acfa96448939b87a924d5e9b21b"
     }
    },
    "5fe50a58f76b41c8b86e0afd261b1256": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70690091fcbc4275b82628436f928099": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79569c2c73a4443fb88d5da778cdd1e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89b6466e829c4f94862128d44378b31e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fd3d2d00eec45cebbf3d3e6da01e4c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a31f7fb3926b43b79b9c18bde98c5f6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf0a5c2f9cbb495aad226dfc100edcde",
      "placeholder": "​",
      "style": "IPY_MODEL_9fd3d2d00eec45cebbf3d3e6da01e4c2",
      "value": " 626/626 [10:40&lt;00:00,  1.10it/s]"
     }
    },
    "aaf7400bc4d5466d954bc6443928f28b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae6f6fa5901d4095a9f63a179efdd194": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b23c8acfa96448939b87a924d5e9b21b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7c6238df06f40f2a0ebdde9b8f4f493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c13fcabcda6a4522bfef0ef9fdfd4b81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce9b732d3fe74065b8e20cd69591d962": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7c6238df06f40f2a0ebdde9b8f4f493",
      "max": 626,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e15e39dafbad46559c9f5c8344743a31",
      "value": 626
     }
    },
    "cf0a5c2f9cbb495aad226dfc100edcde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da1b7397777944b48f031a35c950aa5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70690091fcbc4275b82628436f928099",
      "placeholder": "​",
      "style": "IPY_MODEL_1c9f6f3ee5ea4205b99176242eb26e3a",
      "value": " 347/347 [04:57&lt;00:00,  1.20it/s]"
     }
    },
    "e15e39dafbad46559c9f5c8344743a31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e647ddadf6634f90a8459937f0b7a15c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed0b284af0354e368980338653ac7de3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaf7400bc4d5466d954bc6443928f28b",
      "max": 347,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c13fcabcda6a4522bfef0ef9fdfd4b81",
      "value": 347
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
